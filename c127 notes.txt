we will
learn about Web-Scraping,

Web-scrapping is where we
will write a program that can fetch all
the data from NASA's website for us.

In this website, there
is data present for about 4,277
exo-planets in 428 pages. We will
later use this data to perform analysis
and do deep study using machine
learning, etc.


CREATE ENVIRONMENT:-
python3.8 -m venv venv

ACTIVATE venv:-
venv\Scripts\activate.bat

install the modules:-
pip install bs4
pip install selenium


bs4 (BeautifulSoup) is a python
module, which is famously used for
parsing text as HTML and then
performing actions in it, such as
finding specific HTML tags with a
particular class/id, or listing out all the
li tags inside the ul tags.

Selenium, on the other hand, is used
to interact with the webpage. It is
famously used for automation testing,
such as testing the functionality of a
website (Login/Logout/etc.) but can
be also used to interact with the page
such as clicking a button, etc.Selenium opens up 
the webpage in a browser.


let’s open the link we want to
scrape with Chrome browser using
Selenium -

START_URL ="https://exoplanets.nasa.gov/exoplanet-catalog/"

Now, what is chromedriver here? It’s
a driver that will help us open chrome
browser with Selenium. For that, we
will have to download it from the
following link -

https://chromedriver.chromium.org/do
wnloads

check your chrome version, go to help in settings and check


create a function called scrape()
and inside that function, I will add a
list called 
 headers 

 planet data -



